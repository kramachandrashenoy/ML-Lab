{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3656e14",
   "metadata": {},
   "source": [
    "# ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1dd4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3 Algorithm Results:\n",
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Define the node structure for the decision tree\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature  # Feature index to split on\n",
    "        self.threshold = threshold  # Threshold value for binary splitting\n",
    "        self.left = left  # Left subtree\n",
    "        self.right = right  # Right subtree\n",
    "        self.value = value  # Majority class value for leaf nodes\n",
    "\n",
    "# Calculate the entropy of a given dataset\n",
    "def calculate_entropy(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    entropy = sum(-p * math.log2(p) for p in probabilities)\n",
    "    return entropy\n",
    "\n",
    "# Calculate the information gain for a given split\n",
    "def calculate_information_gain(X, y, feature, threshold):\n",
    "    left_indices = X[:, feature] < threshold\n",
    "    y_left = y[left_indices]\n",
    "    y_right = y[~left_indices]\n",
    "    \n",
    "    entropy_parent = calculate_entropy(y)\n",
    "    entropy_left = calculate_entropy(y_left)\n",
    "    entropy_right = calculate_entropy(y_right)\n",
    "    \n",
    "    weight_left = len(y_left) / len(y)\n",
    "    weight_right = len(y_right) / len(y)\n",
    "    \n",
    "    information_gain = entropy_parent - (weight_left * entropy_left + weight_right * entropy_right)\n",
    "    return information_gain\n",
    "\n",
    "# Recursively build the decision tree\n",
    "def build_tree(X, y, max_depth):\n",
    "    # If all labels are the same or maximum depth reached, create a leaf node\n",
    "    if len(set(y)) == 1 or max_depth == 0:\n",
    "        value = max(set(y), key=list(y).count)\n",
    "        return Node(value=value)\n",
    "    \n",
    "    n_features = X.shape[1]\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_info_gain = -1\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            info_gain = calculate_information_gain(X, y, feature, threshold)\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    if best_info_gain == 0:\n",
    "        value = max(set(y), key=list(y).count)\n",
    "        return Node(value=value)\n",
    "    \n",
    "    left_indices = X[:, best_feature] < best_threshold\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[~left_indices], y[~left_indices]\n",
    "    \n",
    "    left_subtree = build_tree(X_left, y_left, max_depth - 1)\n",
    "    right_subtree = build_tree(X_right, y_right, max_depth - 1)\n",
    "    \n",
    "    return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "# Make predictions using the built tree\n",
    "def predict_tree(tree, x):\n",
    "    if tree.value is not None:\n",
    "        return tree.value\n",
    "    \n",
    "    if x[tree.feature] < tree.threshold:\n",
    "        return predict_tree(tree.left, x)\n",
    "    else:\n",
    "        return predict_tree(tree.right, x)\n",
    "\n",
    "# Evaluate the accuracy of the tree on a test set\n",
    "def evaluate_tree(tree, X_test, y_test):\n",
    "    y_pred = [predict_tree(tree, x) for x in X_test]\n",
    "    accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "    return accuracy\n",
    "\n",
    "# Load the weather dataset from the local file\n",
    "file_path = r\"C:\\Users\\Ramachandra\\OneDrive\\Desktop\\ML Lab\\weather_forecast.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocessing: convert categorical variables to numerical using one-hot encoding\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Splitting the dataset into features and target variable\n",
    "X = df.drop('Play_Yes', axis=1)  # Modify the column name here if needed\n",
    "y = df['Play_Yes']  # Modify the column name here if needed\n",
    "\n",
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the decision tree using our implementation\n",
    "tree_id3 = build_tree(X_train, y_train, max_depth=5)\n",
    "\n",
    "# Evaluate the tree on the test set\n",
    "accuracy_id3 = evaluate_tree(tree_id3, X_test, y_test)\n",
    "print(\"ID3 Algorithm Results:\")\n",
    "print(f\"Accuracy: {accuracy_id3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db047f",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c2eec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART Algorithm Results:\n",
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Define the node structure for the decision tree\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature  # Feature index to split on\n",
    "        self.threshold = threshold  # Threshold value for binary splitting\n",
    "        self.left = left  # Left subtree\n",
    "        self.right = right  # Right subtree\n",
    "        self.value = value  # Majority class value for leaf nodes\n",
    "\n",
    "# Calculate the Gini impurity of a given dataset\n",
    "def calculate_gini(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    gini = 1 - sum(p**2 for p in probabilities)\n",
    "    return gini\n",
    "\n",
    "# Calculate the Gini impurity gain for a given split\n",
    "def calculate_gini_gain(X, y, feature, threshold):\n",
    "    left_indices = X[:, feature] < threshold\n",
    "    y_left = y[left_indices]\n",
    "    y_right = y[~left_indices]\n",
    "    \n",
    "    gini_parent = calculate_gini(y)\n",
    "    gini_left = calculate_gini(y_left)\n",
    "    gini_right = calculate_gini(y_right)\n",
    "    \n",
    "    weight_left = len(y_left) / len(y)\n",
    "    weight_right = len(y_right) / len(y)\n",
    "    \n",
    "    gini_gain = gini_parent - (weight_left * gini_left + weight_right * gini_right)\n",
    "    return gini_gain\n",
    "\n",
    "# Recursively build the decision tree using CART algorithm\n",
    "def build_tree_cart(X, y, max_depth):\n",
    "    if len(set(y)) == 1 or max_depth == 0:\n",
    "        value = max(set(y), key=list(y).count)\n",
    "        return Node(value=value)\n",
    "    \n",
    "    n_features = X.shape[1]\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_gini_gain = -1\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            gini_gain = calculate_gini_gain(X, y, feature, threshold)\n",
    "            if gini_gain > best_gini_gain:\n",
    "                best_gini_gain = gini_gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    if best_gini_gain == 0:\n",
    "        value = max(set(y), key=list(y).count)\n",
    "        return Node(value=value)\n",
    "    \n",
    "    left_indices = X[:, best_feature] < best_threshold\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[~left_indices], y[~left_indices]\n",
    "    \n",
    "    left_subtree = build_tree_cart(X_left, y_left, max_depth - 1)\n",
    "    right_subtree = build_tree_cart(X_right, y_right, max_depth - 1)\n",
    "    \n",
    "    return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "# Make predictions using the built tree\n",
    "def predict_tree(tree, x):\n",
    "    if tree.value is not None:\n",
    "        return tree.value\n",
    "    \n",
    "    if x[tree.feature] < tree.threshold:\n",
    "        return predict_tree(tree.left, x)\n",
    "    else:\n",
    "        return predict_tree(tree.right, x)\n",
    "\n",
    "# Evaluate the accuracy of the tree on a test set\n",
    "def evaluate_tree(tree, X_test, y_test):\n",
    "    y_pred = [predict_tree(tree, x) for x in X_test]\n",
    "    accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "    return accuracy\n",
    "\n",
    "# Load the weather dataset from the local file\n",
    "file_path = r\"C:\\Users\\Ramachandra\\OneDrive\\Desktop\\ML Lab\\weather_forecast.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocessing: convert categorical variables to numerical using one-hot encoding\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Splitting the dataset into features and target variable\n",
    "X = df.drop('Play_Yes', axis=1)  # Modify the column name here if needed\n",
    "y = df['Play_Yes']  # Modify the column name here if needed\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the decision tree using CART algorithm\n",
    "tree_cart = build_tree_cart(X_train, y_train, max_depth=5)\n",
    "\n",
    "# Evaluate the tree on the test set\n",
    "accuracy_cart = evaluate_tree(tree_cart, X_test, y_test)\n",
    "print(\"CART Algorithm Results:\")\n",
    "print(f\"Accuracy: {accuracy_cart}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a817d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
